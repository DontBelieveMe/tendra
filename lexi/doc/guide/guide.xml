<?xml version="1.0" standalone="no"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
"http://www.oasis-open.org/docbook/xml/4.4/docbookx.dtd">

<!--
  $Id$
-->

<book>
  <bookinfo>
    <title>The Lexi users' guide</title>

	<author>
		<firstname>Katherine</firstname>
		<surname>Flavel</surname>
		<affiliation><orgname>The TenDRA Project</orgname></affiliation>
	</author>
	<pubdate>2007</pubdate>

	<copyright>
		<year>2007</year>

		<holder>The TenDRA Project</holder>
	</copyright>
  </bookinfo>

<!-- TODO when writing manpage, we can take the oppertunity to declare as BUGS
(or indeed as undefined) features which no .lxi file uses.
-->

	<!--
	   - I have 1. designed the structure of my document, 2. read the
	   - program's grammar and described each command in turn, 3. begun
	   - integrating a manpage for invocation kevin reverse-engieered, and
	   - will 4. describe the algorithims and runtime it employs
	   -
	   - Don't forget to explain that this was inspired by Kevin's manpage
	   - (and indeed that we'll produce a new manpage from this guide),
	   - although his text was completley rewritten for this guide, it
	   - served as a helpful comparison.
	  -->

	<chapter id="introduction">
		<title>Introduction</title>

		<para><application>Lexi</application> translates a description of a lexical
			analyser into C code implementing that analyser. It aims to
			provide simple, straightforward features for lexical analysis,
			and to leave more complex behaviour to its calling program.</para>

		<para>This document describes how to use the <application>Lexi</application>
			lexer generator. It was written for <application>Lexi</application> version
			1.3.</para>
		<!-- TODO 1.4 for new API changes (call it 2.0 instead) -->

		<para>This version of <application>Lexi</application> (1.3) is an interim to
			mark a stable release before forthcoming <acronym>API</acronym>
			changes scheduled for the next release. This provides no features
			over 1.2 save for some minor points (listed under the change
			history below); it serves mostly to collate the restructuring
			of the codebase during development; 1.3 is the first release of
			<application>Lexi</application> as a stand-alone product, seperate to the
			TenDRA distribution.</para>

		<para>Any origional documentation <application>Lexi</application> once had
			previous to version 1.3 is lost. Since there is no surviving
			documentation, this guide has been written from scratch, based
			from observations of the source code. It is
			difficult to distinguish between obscure features and undefined
			beaviour; we are taking the oppertunity here to explicitly
			state which features are undefined behaviour and which are
			intentional. We try to maintain backwards compatibility with
			these features, based on the <application>Lexi</application> files
			present in the TenDRA repository.</para>

		<para>Features of version 1.3 which are due to be changed for
			the next release are therfore marked depreciated.</para>

		<!-- TODO after our initial release, change this API.
			The initial release is just incase there is anybody
			using this (hah, likely...) -->
		<!-- TODO since we know all changes in advance, state
			which are depreciated -->
		<!-- TODO state why we're changing our api: see the
			"this is for version X" heading -->
		<!-- TODO be sure to state the first release is for backwards
			compatibility only. Make a sequence of API changes. Infact,
			all releases for all products should have such a sequence
			(i.e. a changelog), especially for APIs. -->

		<para><!-- TODO history --></para>

		<section id="history">
			<title>Change History</title>

			<para>The main feature changes of each version of
				<application>Lexi</application> are listed below:

				<!-- TODO 2.0 (?) - forthcoming API changes -->
				<!-- TODO merge this with an external changelog, as for sid -->
				<variablelist>
					<title>Versions of <application>Lexi</application></title>

					<varlistentry>
						<term>1.3</term>
						<listitem>
							<para>Interim convenience release.
		<!-- TODO mostly similar to the lexi in TenDRA 4.2.1,
			save for the addition of // comments (?), and what else?
			- TODO no, it looks like i didn't add kevin's patch for that. -->

								<itemizedlist>
									<listitem>
										<para>Seperated from the TenDRA
											distribution to form a stand-alone
											project.</para>
									</listitem>
									<listitem>
										<para>Various behaviours undefined<!-- TODO --></para>
									</listitem>
								</itemizedlist>
							</para>
						</listitem>
					</varlistentry>

					<varlistentry>
						<term>1.2</term>
						<listitem>
							<!-- Tagged for internal use only -->
		<!-- TODO 1.2 major change in output from 1.2 is lack of PROTO generation -->
							<para><code>PROTO</code> removed<!-- TODO --></para>
						</listitem>
					</varlistentry>

					<varlistentry>
						<term>1.2</term>
						<listitem>
							<para><!-- TODO -->The version provided in TenDRA 4.2.1?</para>
						</listitem>
					</varlistentry>
				</variablelist>

			</para>
		</section>
	</chapter>

	<chapter id="lexing">
		<title>Lexical Analysis</title>

		<section id="lexingoverview">
			<title>Overview</title>

			<para>Lexical analysis is the process of categorising an input
				stream into tokens. Each of these categories (or lexemes)
				are determined by the spelling of the contents of that token.
				For convenience of discussion, this guide assumes that
				tokens are formed of characters, although this is not
				neccessarily the case.</para>

			<para>Categories of tokens are arranged such that spellings the
				possible sets of spellings forming each category are
				unambigious. For example, commonly floating point literals are
				distinguishiable from integer literals because they must contain
				a decimal point. If two different concepts must be expressed
				using the same (or overlapping) sets of spellings, a further
				stage after lexical analysis would be required to distinguish
				them. For example, commonly an identifier may represent either
				a variable or a function name, and it is the role of a parser
				to distinguish between them based on the ordering of the
				tokens.</para>

			<para>Usually it is the responsibility of the lexical analyser
				to skip whitespace (and often also comments).
				<xref linkend="tokenstream"/> illustrates an example of
				tokenisation:</para>

			<figure id="tokenstream">
				<title>Forming a token stream</title>
				<graphic align="center" format="PNG"
					fileref="tokens.png"/>
			</figure>

			<para>In this example, the lexer skips over whitespace characters
				(indicated in the input text by grey with a dotted underline),
				and categorises the remaining characters (indicated with a cyan
				background and a solid underline) according to their spelling. For
				example, a series of digits is categorised into the
				<literal>&lt;literal&gt;</literal> type. Notice that whitespace
				is optional; there happens to be non between the
				<quote><literal>53</literal></quote> and
				<quote><literal>;</literal></quote> tokens.</para>

			<para>The spelling of the characters which form each token is
				passed on, but usually ignored for punctual tokens such
				as statement seperators and assignment operators. It is
				more relevant for the spelling to be known for
				identifiers, literals and other non-punctual tokens.</para>

			<para>The traditional use for a lexer is to feed this stream
				of categorised tokens to a parser; lexers are unaware of the
				order of tokens, since they are concerned with
				categorisation only.  The process of parsing then asserts
				that these tokens are in an order permitted by
				the given grammar. Usually storing the spellings associated with each
				tokens is a side-effect of the parse, resulting in a
				symbol table or equivalent (this depends on the purpose
				of the parser). Parsing is beyond the scope of this document,
				and will not be discussed here - see the Sid users' guide
				for details.</para>

			<para>For simple uses, a parser may not be required at
				<!-- TODO make examples, and mention them here -->all.</para>
		</section>

		<section id="concepts">
			<title>Concepts Lexi Expresses</title>

			<!-- TODO -->
			<variablelist>
				<varlistentry>
					<term><xref linkend="keyworddefinitions"/></term>

					<listitem>
						<para>Keywords are sequences of inseperable characters
							treated whole, indented to be passed back to the
							calling program as single tokens. These differ from
							most tokens in that their spellings are fixed, rather
							than a set of possible spellings.</para>

						<para><application>Lexi</application> itself does not perform
							processing of keywords; instead it outputs their
							definitions seperately (see the <option>-k</option>)
							and these definitions are intended to be checked
							by a token calling a function.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><xref linkend="prepassmappings"/></term>

					<listitem>
						<para>For mappings of sequences of characters to a
							single character, pre-pass substitutions offer
							a mechanism to implement effects similar to
							trigraphs in C.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><xref linkend="whitespacedefinitions"/></term>

					<listitem>
						<para>Whitespace serves both to seperate tokens whos
							concaternated spellings would be ambigious (e.g. to
							dinstinguish <quote>abc</quote> and
							<quote>xyz</quote> from  <quote>abcxyz</quote>),
							and primarily to improve readability of the source.
							Whitespace is not passed on in the generated token
							stream.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><xref linkend="groupdefinitions"/></term>

					<listitem>
						<para>TODO</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><xref linkend="tokendefinitions"/></term>

					<listitem>
						<para>TODO</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><xref linkend="conditionalcommands"/></term>

					<listitem>
						<para>TODO</para>
					</listitem>
				</varlistentry>
			</variablelist>

			<para>Any other facilities are provided by matching the start of
				a token and passing control to an externally defined function,
				This seperation provides a general mechanism to allow the user
				to handle situations unforseen by the authors, without making
				<application>Lexi</application> itself overcomplex by attempting to provide
				built-in mechanisms to express every corner-case (for example,
				switching to a "comment-parsing" mode). See the <!-- TODO -->
				functioncalls section for details.</para>
		</section>


<!-- TODO in a future version
		<section id="implementation">
			<title>Lexi's Implementation</title>

			<para>Here we talk about how the generated lexical analysers
				work; how they use a look-up table for speed, etc. Note that
				they read a byte at a time: perhaps mention the ignorance of
				unicode. TODO feature request: perhaps set unicode (or other
				chrset mapping) to UTF-8 as a pass phase<para>

			<para>TODO talk about passes </para>

			<para>TODO see section on performance.</para>
		</section>
-->

		<section id="invocation">
			<title>Invocation</title>

			<para><application>Lexi</application> is invoked with the form:</para>

			<xi:include xmlns:xi="http://www.w3.org/2001/XInclude"
				href="../cmdsynopsis.xml"/>

			<para>The <replaceable>output-file</replaceable> written is
				either an implementation of the
				lexical analyser specified, or if <option>-k</option> is
				given, a set of calls to define the keywords used.</para>

			<para>The <replaceable>input-file</replaceable> is the
				specification of the lexical analyser to generate.
				See <xref linkend="specification"/> for details of the
				format of this file.</para>

			<para>The <option>-l</option> parameter specifies a prefix
				to the identifiers of tokens returned; this defaults to
				<literal>lex_</literal>.</para>

			<para>For details of the options available, see the
				<application>Lexi</application> manpage.</para>
		</section>
	</chapter>

	<xi:include xmlns:xi="http://www.w3.org/2001/XInclude"
		href="specification.xml"/>

	<xi:include xmlns:xi="http://www.w3.org/2001/XInclude"
		href="commands.xml"/>

	<chapter id="interface">
		<!-- TODO in the future this should be seperate chapters, one per language supported -->
		<title>Interface</title>

		<para><application>Lexi</application> generates and outputs C89 code
			implementing the specified lexical analyser. This code
			is intended to be linked as part of a larger C program
			by the user.</para>

		<!-- TODO state size limits of things in a "limits" section -->

		<section>
			<title>Instantiation</title>

			<para>Input to the generated lexical analyser is by way of
				two functions (or macros, if desired) that the user is
				expected to provide. These read a character and un-read
				a character respectivley:</para>

			<programlisting language="C">int read_char(void);
void unread_char(int);</programlisting>

			<para>Output from the lexical analyser is by way of a function
				provided by <application>Lexi</application> that returns the next
				token to the user's program:</para>

			<programlisting language="C">int read_token(void);</programlisting>

			<para>This is the user's primary interface to the lexical
				analyser. Note that the type of characters is <code>int</code>,
				so that <literal>EOF</literal> may be expressed if
				neccessary.</para>

			<para>Calling <code>read_token()</code> will result
				in the lexical analyser
				making as many calls to <code>read_char()</code>
				and <code>unread_char()</code> as are neccessary.
				The <code>unread_char()</code> function may be called
				consecutivley whilst matching tokens, and so a
				buffer the size of the longest token is the
				maximum length required.</para>
		</section>

		<section id="interface_terminals">
			<title>Terminals</title>

			<para><application>Lexi</application> does not define the values of
				terminals in the generated code; these are expected to
				be specified by the user (most usually from a
				parser-generator such as <application>Sid</application>).
				For example, a token defined by:</para>

			<programlisting language="Lexi">TOKEN "--" -> $dash ;</programlisting>

			<para>Would return the value of <literal>lex_dash</literal>
				from <code>read_token()</code> when matched. The prefix of
				these identifier names may be specified with the
				the <option>-l</option> option.
				See the <application>Sid</application> documentation for
				further discussion of the C representation of
				<application>Sid</application>'s terminals.</para>

			<!-- TODO state the API. ditto sid. include macros etc. (done?) -->

			<!-- TODO talk about how the actual values are returned
				with tokens (tags); they're not. you're expected to
				do so yourself -->
		</section>

		<section id="interface_functions">
			<title>Functions</title>
			<!-- TODO we could express system-defined functions as $x() perhaps -->

			<para>Within the C implementation of functions, the usual
				<application>Lexi</application> <acronym>API</acronym>
				functions may be called. For example, to call
				<code>read_char()</code> and <code>unread_char()</code>.
				This is especially useful for calling the functions
				defined to identify membership in groups. A common
				case is to read tokens of a variable length.
				This is especially suitable for reading identifiers.
				For example:

				<programlisting language="Lexi">GROUP identstart = {a-z} + {A-Z} + "_";
GROUP identbody = "[identstart]" + {0-9} + "-";
TOKEN "[identstart]" -> read_identifier();</programlisting>

			<programlisting language="C">int read_identifier(int c) {
	for(;;) {
		if(c == EOF) {
			return lex_eof;
		}

		if(!is_identbody(lookup_char(c))) {
			unread_char(c);
			return lex_identifier;
		}

		/* store character here */

		c = read_char();
	}
}</programlisting>

				It is important to honour the usual <acronym>API</acronym>
				constraints; for example, do not call
				<code>unread_char()</code> consequtively more than once<!--
				TODO is that true? does lexi call it more than once? -->.</para>


			<!-- TODO in the future when we remove the lookup_char function, you
				will be able to TOKEN "x" -> is_digit(). I don't know if that
				would be useful, but mention it anyway -->
			<!-- TODO mention the API constraints still hold: don't call
				unread_char() more than once, etc -->

			<para><!-- TODO -->This latter function takes some arguments. The example
				above would result in the call to get_identifier ('t','o',k','e','n') ;
			</para>

<!--
TODO UNDEFINED BEHAVIOUR
			<para>However, be careful when specifying functions called by
				different amounts of characters. For example:</para>

			<programlisting language="Lexi">
TOKEN "//" -> get_comment () ;
TOKEN "#"  -> get_comment () ;</programlisting>

			<para>Would make calls to <code>get_comment('/','/');</code> and
				<code>get_comment('#');</code> respectivley.
				The difference in the number of arguments
				may cause a problem. One workaround is to call
				two macros of different prototypes which call
				the same function.</para>
-->
		</section>

		<section>
			<title>Character Groups</title>

			<para>Should the user wish to check if a character is
				in a group, the generated code provides macros of
				the form <code>is_groupname()</code>. These are
				intended to be used as:</para>

			<programlisting language="C">is_digit(lookup_char(c))</programlisting>

			<para>assuming a group named <quote>digit</quote> is defined. See the
				<xref linkend="groupdefinitions" endterm="groupdefinitions.title"/>
				and <xref linkend="whitespacedefinitions"
				endterm="whitespacedefinitions.title"/> white sections for
				further details on group names.</para>
		</section>

		<section>
			<title>Keywords</title>
<!-- TODO mention MAKE_KEYWORD here. Give an example -->

			<para>Neither the keyword calls output by <option>-k</option> nor
				the lexical analyser itself depend on including any headers
				other than for the user's own code's requirements.</para>
		</section>
	</chapter>

<!-- TODO in a future version. probably discuss the trie here, too (just not the implementation of it)
	<chapter>
		<title>Implementation</title>

		<para>What is generated, and algorithims employed</para>

		<para>Performance relative to other lexers. We can port all our examples to lex and such, to compare.
			Side-by-side comparisons would also help users understand them if they know lex already.</para>
	</chapter>
-->

	<chapter id="appendix">
		<title id="appendix.title">Appendix</title>

		<!-- TODO maybe not in the guide: in an examples/ directory
			perhaps?

		<section>
			<title>Example descriptions</title>

			<section>
				<title>Comment stripping</title>

				<para>A C comment stripper. It needs to be aware of
					string literals (so it can avoid comment delimiters inside them),
					characters constants (ditto) and comments (so it can strip
					them).</para>
			</section>

			<section>
				<title>RPN Calculator</title>
			</section>

			<section>
				<title>Non-RPN Calculator</title>

				<para>TODO joint example with sid.</para>
			</section>
		</section>
		-->

		<!--
		<section>
			<title>Description grammar</title>
			Grammar (for reference) and sequence of keywords, reserved words, etc
				This probably makes more sense for the next version, really
		</section>
		-->

		<section>
			<title>Undefined behaviour</title>

			<!-- TODO list undefined behaviours -->

			<!--
				and so it is an error to define a mapping which produces
				a character used at the start of any mapping, including
				itself. For example:</para>
				TODO to identify this, we add a stage after the table is
				constructed which checks each mapping.
				TODO new undefined behaviour -->
		</section>

		<section>
			<title>Obscure Features</title>

			<para>This section describes legal constructs of doubtful
				interest.</para>

			<!-- TODO kevin says:
            TOKEN "&amp;&amp;" -> and;

       are legal tokens' definitions. The first one lacks the $ sign and is of
       no  interest  if  you use the sid parser generator, and lexi is usually
       meant to be used with sid.
	TODO See above re lex_
	This is reundant now that we're going to disallow non-sid identifiers. When we do, we should rename them.
			-->

			<section>
				<title>Ranges within tokens</title>

				<para>The grammar for <application>Lexi</application> permits
					strings formed of pre-defined character ranges to be used
					for token definitions:

					<programlisting language="Lexi">TOKEN {A-Z} + "something" -> $x ;</programlisting>

					Character ranges are intended
					to be used for sets (that is, in <code>GROUP</code>), not
					sequences. Since tokens are defined using sequences, this
					really means:

					<programlisting>TOKEN "ABCDEFGHIJKLMNOPQRSTUVWXYZsomething" -> $x ;</programlisting>
				</para>

				<para>This is probably not the intended effect.<!-- TODO dissalow
					ranges for tokens, then! eventually we can remove from
					grammar. dissalow after first release. --> Using a group
					is suggested instead:

					<programlisting>GROUP alpha = {A-Z} ;
TOKEN "[alpha]something" -> $x ;</programlisting>
				</para>
			</section>

			<section>
				<title>Whitespace within tokens and keywords</title>

				<para>The <quote>white</quote> group may be used in
					tokens and keywords as any other group would:</para>

				<programlisting language="Lexi">
TOKEN "a[white]" -> $something ;
TOKEN "[white]ab" -> $neverscanned ;</programlisting>

				<para>The above are both legal tokens definitions.
					The second one will  never  be  scanned since
					white characters are discarded before tokens are
					matched.</para>	<!-- TODO in the future make this
					illegal. -->

				<!--
				TODO the first one is ok? odd! TODO list precendence of these
				things! that should be a section by itself perhaps
				-->
			</section>
		</section>
	</chapter>

	<!-- TODO include this in the global glossary as a per-project section -->
	<chapter id="glossary">
		<title>Glossary</title>

<!--
Token
Group
Set
List
Keyword
String
Range
-->
	</chapter>
</book>

