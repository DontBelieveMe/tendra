<?xml version="1.0" standalone="no"?>
<!DOCTYPE book SYSTEM "minidocbook.dtd" [
	<!ENTITY lexi.app "<application>Lexi</application>">
]>

<!-- $Id$ -->

<book>

	<bookinfo>
		<title>The &lexi.app; Users' Guide</title>

		<authorgroup>
			<author>
				<firstname>Katherine</firstname>
				<surname>Flavel</surname>
				<affiliation><orgname>The TenDRA Project</orgname></affiliation>
			</author>

			<author>
				<firstname>Kevin</firstname>
				<!-- surname elided -->
				<affiliation><orgname>The TenDRA Project</orgname></affiliation>
			</author>

			<author>
				<firstname>Rob</firstname>
				<surname>Andrews</surname>
				<affiliation><orgname>DERA</orgname></affiliation>
				<contrib>Original author of Lexi</contrib>
			</author>
		</authorgroup>

		<pubdate>2007</pubdate>

		<copyright>
			<year>2014</year>
			<year>2012</year>
			<year>2011</year>
			<year>2010</year>
			<year>2009</year>
			<year>2008</year>
			<year>2007</year>
			<holder>The TenDRA Project</holder>
		</copyright>

		<!-- TODO: remove obsoleted parts as each feature is changed -->

		<revhistory>
			<revision role="featurechange">
				<date>2014-04-24</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Combined C99 and C90 code generation. The applicable
						API is now selected by `#if` on `__STDC_VERSION__`.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2014-04-24</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Removed <code>-a</code>; I can't think of any reason why you might
						want to generate standard C code eliding assertions, as opposed to
						compiling with `-DNDEBUG`. So now assertions are always generated.</para>
				</revdescription>
			</revision>

			<revision role="dev">
				<date>2012-04-13</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Reworked existing vector images for consistency
						between documents.</para>
				</revdescription>
			</revision>

			<revision role="buildsystem">
				<date>2010-07-29</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>The dependency on libexds is now resolved internally
						for convenience of building.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2009-04-24</date>
				<authorinitials>kevin</authorinitials>
				<revdescription>
					<para>Removed <code>##</code>.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-12-01</date>
				<authorinitials>kevin</authorinitials>
				<revdescription>
					<para>Implemented group merge.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-11-17</date>
				<authorinitials>kevin</authorinitials>
				<revdescription>
					<para>Added <code>ARGUMENT name:ctype;</code></para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-11-02</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>EOF is no longer permitted in groups.</para>

					<para>Empty groups are not present in the lookup table.</para>

					<para>Groups may now contain character <code>\0</code>.</para>

					<para>Group inversion is supported for including subsets:
						<code>GROUP a = "[^b]";</code></para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-09-26</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Removed legacy support for tokens mapping to functions (and in particular,
						literally-embedded fragments of source code specified as string literals for
						function arguments). These were written in the form:</para>

					<programlisting language="lxi">TOKEN "abc" -&gt; f(...);</programlisting>

					<para>with various arguments permitted inside the parenthesies.
						These have been superseded by SID-like actions defined in the
						<filename>.lct</filename> file, which look like:</para>

					<programlisting language="lxi">TOKEN "abc" -&gt; &lt;f&gt;(...);</programlisting>

					<para>These provide a language-agnostic manner to achieve the same effect,
						keeping all language-specific portions in the <filename>.lct</filename>. file.</para>

					<para>Any existing <filename>.lxi</filename> files ought to be updated to
						use these with an accompanying <filename>.lct</filename> file, instead.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-07-10</date>
				<authorinitials>kevin</authorinitials>
				<revdescription>
					<para>Added the &ldquo;test&rdquo; language.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-06-24</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>The <filename>.lct</filename> headers are now output after
						the generated lexer's header is included.
						This is a convenience to permit the <filename>.lct</filename>
						headers to make use of symbols from the generated header
						(notablly <code>LEXI_EOF</code> in static support functions,
						for example).</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-06-24</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Added a new command line option, <code>-i</code>, for
						specifying a prefix for user-supplied interface functions
						separately from the <code>-p</code> prefix.
						Currently the only such user-supplied interfaces are
						<code>*getchar()</code> and <code>*unknown_token</code>.</para>

					<para>The rationalle for introducing this option is to permit
						these interfaces to be shared from a common source whilst keeping
						the <code>-p</code> prefixes separate, as would be the case for
						a <filename>.lct</filename> file shared between many lexers each
						with their own <filename>.lxi</filename> specification.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-05-25</date>
				<authorinitials>kevin</authorinitials>
				<revdescription>
					<para>Added <filename>.lct</filename> actions files.
						These are an analogue to &sid;'s <filename>.act</filename>
						files.</para>

					<para>Four predefined types have been added: <code>TERMINAL</code>,
						<code>INTEGER</code>, <code>STRING</code> and <code>CHARACTER</code>.
						Scopes (instructions_list) can now retain local names with their
						types.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-01-22</date>
				<authorinitials>kevin</authorinitials>
				<revdescription>
					<para>Added <code>[...)</code> range markers for zones.
						This is mostly useful for identifiers.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-01-09</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Added Graphivz &ldquo;Dot&rdquo; output.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-01-08</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para><code>lexi_group()</code> now returns a <code>bool</code>
						for C99.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-01-08</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Added <code>-a</code>; assertions are now disabled by default.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-01-08</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Added C99 as an output language.</para>
				</revdescription>
			</revision>

			<revision role="dev">
				<date>2008-01-08</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Added support for multiple output files,
						specific to each output language available.
						The meaning of these files is language-specific.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-01-05</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Unknown tokens are now provided by
						<code>lexi_unknown_token</code>, which forms part of the generated API.
						The character passed is removed, as it serves little purpose.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2008-01-05</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para><code>LEX_EOF</code> is now known as
						<code>LEXI_EOF</code>.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-11-25</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Moved the token buffer into lexi_state.
						This introduces state to the buffer-manipulation API,
						which breaks backwards compatibility.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-11-24</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>The state for zones is now maintained as part of a slightly
						more formal interface; unfortunately the contents of this struct
						(though intended to be private) are visible for the convenience
						of allocation, but defining an instance of this struct on the
						user's side of the generated API allows for multiple instances
						of Lexi to run concurrently.</para>

					<para>The API is now consistent for both with and without zones.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-11-24</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Added -l for the output language.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-11-24</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>The group-querying functions are now behind a single interface,
						to which an enumeration of group identifiers is passed.
						This breaks compatibility for <code>is_*()</code>, which no longer exist.
						This helps provide a static set of functions should somebody wish to
						wrap calls to lexi-generated code; the contents of the enumeration
						scale with groups, rather than the size of the API.</para>

					<para>This change brings the lookup table behind the API, making it static.
						The type of the lookup table is also removed from the public interface.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-11-22</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Reworked prefix generation.
						All generated symbols should now be
						prefixed, which breaks compatibility.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-11-22</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Removed single-file output; the generated header is now
						mandatory.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-11-21</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Implemented an alternate keywords mechanism.
						This introduces a new function, <code>lexi_keyword()</code>,
						which returns an int representing the keyword found.
						The intention is that this would share a token ID along
						with Lexi's main interface.</para>

					<para>Removed the -k option.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-11-19</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Removed <code>lookup_char()</code> in favour of passing the
						character directly to the <code>is_*()</code> macros.
						This is an API change.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-10-15</date>
				<authorinitials>kevin</authorinitials>
				<revdescription>
					<para>Added the -p prefix option.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-10-14</date>
				<authorinitials>kevin</authorinitials>
				<revdescription>
					<para>Added <code>COPYRIGHT</code> and the <code>-C</code>
						option for specifying copyright files on the command
						line. The origional &ldquo;first comment&rdquo; behaviour
						is kept, if neither of these are specified.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-10-11</date>
				<authorinitials>kevin</authorinitials>
				<revdescription>
					<para>Added <code>[^...]</code> for complements of groups.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-10-10</date>
				<authorinitials>kevin</authorinitials>
				<revdescription>
					<para>Added <code>#n</code> and a literal string for function
						arguments.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-10-10</date>
				<authorinitials>kevin</authorinitials>
				<revdescription>
					<para>Added optional support for a generated C header file.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-10-05</date>
				<authorinitials>kevin</authorinitials>
				<revdescription>
					<para>Added support for groups in zones.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-09-24</date>
				<authorinitials>kevin</authorinitials>
				<revdescription>
					<para>Added zones.</para>

<!-- TODO: explain what zones are-->
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-09-15</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Merged in support for up to (and including) 31 groups.
						This is a rework of a patch origionally submitted by
						Rob Andrews, but not included in our tag for Lexi 1.2.
						Notably it has been adapted to use stdint.h
						instead of assuming the size of various C types.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-08-10</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Lexi now maintains its own internal fixed-length buffer
						when reading tokens.
						This obsoletes the <code>unread_char()</code> function,
						which is now no longer called by lexi.</para>

					<para>This change aims to be backwards-compatible; it should
						not affect existing programs, though their
						<code>unread_char()</code> functions may now be redundant
						and can be removed if not used by their own routines.</para>
				</revdescription>
			</revision>

			<revision role="version">
				<date>2007-07-29</date>
				<revnumber>Lexi 1.3</revnumber>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Tagged Lexi 1.3.</para>
				</revdescription>
			</revision>

			<revision role="docs">
				<date>2007-07-07</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Wrote the Lexi Users' Guide.</para>

					<para>This was mostly reverse-engineered from the source, and by
						experimenting with Lexi.</para>
					<!-- If you're interested, I wrote it on a vt220 -->
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-06-03</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Added examples.</para>
				</revdescription>
			</revision>

			<revision role="dev">
				<date>2007-07-05</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Converted from OSSG to ANSI C.</para>
				</revdescription>
			</revision>

			<revision role="featurechange">
				<date>2007-04-26</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Added <code>-h</code>.</para>
				</revdescription>
			</revision>

			<revision role="version">
				<date>2007-04-25</date>
				<authorinitials>kate</authorinitials>
				<revdescription>
					<para>Moved out Lexi to a standalone tool.</para>
				</revdescription>
			</revision>

			<revision role="version">
				<date>2004-09-21</date>
				<revnumber>Lexi 1.2</revnumber>
				<authorinitials>asmodai</authorinitials>
				<revdescription>
					<para>Tagged Lexi 1.2.</para>

					<para>This corresponds to Lexi 1.2, which was developed
						privately by Rob after the 4.1.2 TenDRA release.
						We are skipping version 1.2 to avoid confusion with his
						version.</para>
				</revdescription>
			</revision>

			<revision role="version">
				<date>1998-07-30</date>
				<revnumber>Lexi 1.1</revnumber>
				<authorinitials>DERA</authorinitials>
				<revdescription>
					<para>TenDRA 4.1.2 release.</para>
				</revdescription>
			</revision>
		</revhistory>
	</bookinfo>

<!-- TODO: when writing manpage, we can take the opportunity to declare as BUGS
(or indeed as undefined) features which no .lxi file uses.
-->

	<!--
	   - I have 1. designed the structure of my document, 2. read the
	   - program's grammar and described each command in turn, 3. begun
	   - integrating a manpage for invocation Kevin reverse-engineered, and
	   - will 4. describe the algorithms and runtime it employs
	   -
	   - Don't forget to explain that this was inspired by Kevin's manpage
	   - (and indeed that we'll produce a new manpage from this guide),
	   - although his text was completely rewritten for this guide, it
	   - served as a helpful comparison.
	  -->

	<preface id="introduction">
		<title>Introduction</title>

		<para>&lexi.app; translates a description of a lexical
			analyser into C code implementing that analyser. It aims to
			provide simple, straightforward features for lexical analysis,
			and to leave more complex behaviour to its calling program.</para>

		<para>This document describes how to use the &lexi.app;
			lexer generator. It was written for &lexi.app; version
			2.0.</para>

		<para>This version of &lexi.app; (trunk pre 2.0) is the current
			working version.
			Once all new features have been added, we will tag a 2.0-proto-1
			release. The 2.0 release will happen once we have frozen the syntax.
			Backward compatibility is not preserved with regards to 1.3. However,
			we are currently working on an option switch that would emulate old behavior.</para>

		<para>This document is a modification of the userguide for
			&lexi.app; 1.3.
			&lexi.app; 1.3 was an interim release to
			mark a stable release before forthcoming &api;
			changes scheduled for the next release. This provides no features
			over 1.2 save for some minor points (listed under the change
			history below); it serves mostly to collate the restructuring
			of the codebase during development; 1.3 is the first release of
			&lexi.app; as a stand-alone product, separate to the
			TenDRA distribution.</para>

		<para>Any original documentation &lexi.app; once had
			previous to version 1.3 is lost. Since there is no surviving
			documentation, this guide has been written from scratch, based
			from observations of the source code. It is
			difficult to distinguish between obscure features and undefined
			behaviour; we are taking the opportunity here to explicitly
			state which features are undefined behaviour and which are
			intentional. We try to maintain backwards compatibility with
			these features, based on the &lexi.app; files
			present in the TenDRA repository.</para>
	</preface>

	<chapter id="lexing">
		<title>Lexical Analysis</title>

		<section id="lexing-overview">
			<title>Overview</title>

			<para>Lexical analysis is the process of categorising an input
				stream into tokens. Each of these categories (called lexemes)
				are determined by the spelling of the contents of that token.
				For convenience of discussion, this guide assumes that
				tokens are formed of characters, although this is not
				necessarily the case.</para>

<!-- TODO: maybe reword the above?
firstly define lexemes
<kate> the process of categorising words into each lexeme is called lexing
<kate> a program which performs lexing is called a lexer
<kate> this program generates lexers based on your set of lexemes
<kate> clearly defining every word would be quite tedious
<kate> so (say we have a lexeme called "number") you can say like "anything 
       consisting entirely of digits is a number"
-->

			<para>Categories of tokens are arranged such that spellings the
				possible sets of spellings forming each category are
				unambiguous. For example, commonly floating point literals are
				distinguishable from integer literals because they must contain
				a decimal point. If two different concepts must be expressed
				using the same (or overlapping) sets of spellings, a further
				stage after lexical analysis would be required to distinguish
				them. For example, commonly an identifier may represent either
				a variable or a function name, and it is the role of a parser
				to distinguish between them based on the ordering of the
				tokens.</para>

			<para>Usually it is the responsibility of the lexical analyser
				to skip whitespace (and often also comments).
				<xref linkend="token-stream"/> illustrates an example of
				tokenisation:</para>

			<figure id="token-stream">
				<title>Forming a Token Stream</title>
				<graphic align="center"
					fileref="images/tokens.svg"/>
			</figure>

			<para>In this example, the lexer skips over whitespace characters
				(indicated in the input text with a shaded background)
				and categorises the remaining characters according
				to their spelling.
				For example, a series of digits is categorised into the
				<literal>&lt;literal&gt;</literal> type. Notice that whitespace
				is optional; there happens to be none between the
				<quote><literal>53</literal></quote> and
				<quote><literal>;</literal></quote> tokens.</para>

			<para>The spelling of the characters which form each token is
				passed on, but usually ignored for punctual tokens such
				as statement separators and assignment operators. It is
				more relevant for the spelling to be known for
				identifiers, literals and other non-punctual tokens.</para>

			<para>The traditional use for a lexer is to feed this stream
				of categorised tokens to a parser; lexers are unaware of the
				order of tokens, since they are concerned with
				categorisation only.  The process of parsing then asserts
				that these tokens are in an order permitted by
				the given grammar. Usually storing the spellings associated with each
				tokens is a side-effect of the parse, resulting in a
				symbol table or equivalent (this depends on the purpose
				of the parser). Parsing is beyond the scope of this document,
				and will not be discussed here - see the &sid-userguide.doc;
				for details.</para>

			<para>For simple uses, a parser may not be required at
				<!-- TODO: make examples, and mention them here -->all.</para>
		</section>

		<section id="concepts">
			<title>Concepts Lexi Expresses</title>

			<!-- TODO -->
			<variablelist>
				<varlistentry>
					<term><link linkend="keyword-definitions">Keyword Definitions</link></term>

					<listitem>
						<para>Keywords are sequences of inseparable characters
							treated whole, indented to be passed back to the
							calling program as single tokens. These differ from
							most tokens in that their spellings are fixed, rather
							than a set of possible spellings.</para>

						<para>&lexi.app; itself does not perform
							processing of keywords; instead it outputs their
							definitions separately (see the <option>-k</option>)
							and these definitions are intended to be checked
							by a token calling a function.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><link linkend="pre-pass-mappings">Pre-pass Mappings</link></term>

					<listitem>
						<para>For mappings of sequences of characters to a
							single character, pre-pass substitutions offer
							a mechanism to implement effects similar to
							trigraphs in C.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><link linkend="whitespace-definitions">Whitespace Definitions</link></term>

					<listitem>
						<para>Whitespace serves both to separate tokens who's
							concatenated spellings would be ambiguous (e.g. to
							distinguish <quote>abc</quote> and
							<quote>xyz</quote> from  <quote>abcxyz</quote>),
							and primarily to improve readability of the source.
							Whitespace is not passed on in the generated token
							stream.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><link linkend="group-definitions">Group definitions</link></term>

					<listitem>
						<para>Group definitions define sets of characters which
							may be specified in place of a single character
							in tokens. The presence of a character in a group
							may also be tested via the generated
							<acronym>API</acronym>, much like C's
							<filename>ctype.h</filename> functions.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><link linkend="token-definitions">Token definitions</link></term>

					<listitem>
						<para>Token definitions specify the set of spellings
							which form a lexeme. These are central to
							lexical analysis.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><link linkend="default-definition">Default definitions</link></term>

					<listitem>
						<para>Default definitions specify which action
						  should we performed upon encountering a non recognized 
						  token.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><link linkend="zone-definitions">Zone definitions</link></term>

					<listitem>
						<para>Zone definitions specify zones in which
							the lexical analysis is subject to different rules.
							Zones definition may contain token definitions,
							default definition, group definitions and whitespace
							definitions.</para>
					</listitem>
				</varlistentry>
			</variablelist>

			<para>Prior to 2.0, zones and default did not exist and
		        any other facilities was provided by matching the start of
				a token and passing control to an externally defined function,
				This separation provided a general mechanism to allow the user
				to handle situations unforeseen by the authors, without making
				&lexi.app; itself over complex by attempting to provide
				built-in mechanisms to express every corner-case (for example,
				switching to a "comment-parsing" mode). See the <!-- TODO -->
				function calls section for details. In 2.0, zones, at least once buffers are
		        implemented, should allow to express most of these possibililities even
		        ones not foreseen by the authors.</para>
		</section>

<!-- TODO: in a future version
		<section id="implementation">
			<title>Lexi's Implementation</title>

			<para>Here we talk about how the generated lexical analysers
				work; how they use a look-up table for speed, etc. Note that
				they read a byte at a time: perhaps mention the ignorance of
				Unicode. TODO: feature request: perhaps set Unicode (or other
				charset mapping) to UTF-8 as a pass phase<para>

			<para>TODO: talk about passes </para>

			<para>TODO: see section on performance.</para>
		</section>
-->
	</chapter>

	<xi:include xmlns:xi="http://www.w3.org/2001/XInclude"
		href="specification.xml"/>

	<xi:include xmlns:xi="http://www.w3.org/2001/XInclude"
		href="commands.xml"/>

	<chapter id="action-file">
		<title>The lexi action file</title>

		<section>
			<title>Introduction</title>

			<para>The lexi action file (.lct) is the second input file for lexi. In it you
				can specify header and trailer for both of the ouput files. Before the release of 2.0,
				it will also contains the definitions of the inline actions. Syntax is not frozen yet
				but will be prior to the 2.0 tag.</para>
		</section>

		<section>
			<title>The header and trailer statements</title>

			<para>These are optional statements. They are used to specify headers
				and trailers for both output files.
				The header is mostly used to add <code>#include</code> directives in the
				output files, to <code>typedef</code> mappings to lexi's types,
				and other preamble before the generated code.</para>

			<programlisting language="lxi">HEADERS = @{
	/* header for the C file */
@}, @{
	/* Header for the h file */
@};</programlisting>

		<programlisting language="lxi">TRAILERS = @{
	/* Trailer for the C file */
@}, @{
	/* Trailer for the h file */
@};</programlisting>
		</section>

		<section>
			<title>Action definition statement</title>

			<para>The syntax is accepted but the semantics are not fully coded yet.
				An action definition defines the inline code that will be inserted
				upon encountering a token whose instruction list contains this action
				call. Here is an example of the intended syntax</para>

			<programlisting language="lxi">ACTION actionname: (inputvar1 :T1, inputvar2 :T2) -> (outputvar1 :T3) = @{
	/* code block */
@},
</programlisting>
		</section>
	</chapter>

	<chapter id="interface">
		<!-- TODO: in the future this should be separate chapters, one per language supported -->
		<title>Interface</title>

		<para>&lexi.app; generates and outputs C89 code
			implementing the specified lexical analyser. This code
			is intended to be linked as part of a larger C program
			by the user.</para>

		<!-- TODO: state size limits of things in a "limits" section. Exceeding limits is undefined -->

		<section>
			<title>Instantiation</title>

			<para>Input to the generated lexical analyser is by way of
				a single function (or macro, if desired) that the user is
				expected to provide to read a character:</para>

			<programlisting language="c">int read_char(void);</programlisting>

			<para>Output from the lexical analyser is by way of a function
				provided by &lexi.app; that returns the next
				token to the user's program:</para>

			<programlisting language="c">int read_token(void);</programlisting>

			<para>This is the user's primary interface to the lexical
				analyser. Note that the type of characters is <code>int</code>,
				so that <literal>EOF</literal> may be expressed if
				neccessary.</para>

			<para>Calling <code>read_token()</code> will result
				in the lexical analyser
				making as many calls to <code>read_char()</code>
				as are necessary.</para>

    <!-- TODO: talk about read_char_aux() used for MAPPNG.
        read_char_aux() is called in place of read_char().
    -->
		</section>

		<section id="interface-terminals">
			<title>Terminals</title>

			<para>&lexi.app; does not define the values of
				terminals in the generated code; these are expected to
				be specified by the user (most usually from a
				parser-generator such as <application>Sid</application>).
				For example, a token defined by:</para>

			<programlisting language="lxi">TOKEN "--" -> $dash ;</programlisting>

			<para>Would return the value of <literal>lex_dash</literal>
				from <code>read_token()</code> when matched. The prefix of
				these identifier names may be specified with the
				the <option>-l</option> option.
				See the <application>Sid</application> documentation for
				further discussion of the C representation of
				<application>Sid</application>'s terminals.</para>

			<!-- TODO: state the API. ditto sid. include macros etc. (done?) -->

			<!-- TODO: talk about how the actual values are returned
				with tokens (tags); they're not. you're expected to
				do so yourself -->
		</section>

		<section id="interface-functions">
			<title>Functions</title>

			<!-- TODO: we could express system-defined functions as $x() perhaps -->

			<para>Within the C implementation of functions, the usual
				&lexi.app; <acronym>API</acronym>
				functions may be called. For example, to call
				<code>read_char()</code>.
				This is especially useful for calling the functions
				defined to identify membership in groups. A common
				case is to read tokens of a variable length.
				This is especially suitable for reading identifiers.
				For example (where <code>unread_char()</code> is a
				user-defined function with the obvious effect):</para>

			<programlisting language="lxi">GROUP identstart = {a-z} + {A-Z} + "_";
GROUP identbody = "[identstart]" + {0-9} + "-";
TOKEN "[identstart]" -> read_identifier();</programlisting>

			<programlisting language="c">int read_identifier(int c) {
	for (;;) {
		if (c == EOF) {
			return lex_eof;
		}

		if (!is_identbody(lookup_char(c))) {
			unread_char(c);
			return lex_identifier;
		}

		/* store character here */

		c = read_char();
	}
}</programlisting>

			<!-- TODO: in the future when we remove the lookup_char function, you
				will be able to TOKEN "x" -> is_digit(). I don't know if that
				would be useful (since it returns an int), but mention it anyway -->

			<para>Functions called by tokens are passed each character forming the
				token. The example above would result in the call to:</para>

			<programlisting language="c">get_identifier(c);</programlisting>

			<para>where <code>c</code> is the content of the token (that is, the
				character matched by <code>"[identstart]"</code>. For multiple
				characters each character is passed:</para>

			<programlisting language="lxi">TOKEN "abc" -> f();</programlisting>

			<programlisting language="c">get_identifier('a', 'b', 'c');</programlisting>

			<para>See <xref linkend="interface"/> for further details of the
				C interface the generated lexer will call.</para>

			<para>Note that it is undefined behaviour to have tokens of
				different lengths call the same function.</para>

		</section>

		<section>
			<title>Character Groups</title>

			<para>Should the user wish to check if a character is
				in a group, the generated code provides macros of
				the form <code>is_groupname()</code>. These are
				intended to be used as:</para>

			<programlisting language="c">is_digit(lookup_char(c))</programlisting>

			<para>assuming a group named <quote>digit</quote> is defined. See the
				<xref linkend="group-definitions"/>
				and <xref linkend="whitespace-definitions"/> white sections for
				further details on group names.</para>
		</section>

		<section>
			<title>Keywords</title>
<!-- TODO: mention MAKE_KEYWORD here. Give an example -->

			<para>Neither the keyword calls output by <option>-k</option> nor
				the lexical analyser itself depend on including any headers
				other than for the user's own code's requirements.</para>
		</section>
	</chapter>

<!-- TODO: in a future version. probably discuss the trie here, too (just not the implementation of it)
	<chapter>
		<title>Implementation</title>

		<para>What is generated, and algorithms employed</para>

		<para>Performance relative to other lexers. We can port all our examples to lex and such, to compare.
			Side-by-side comparisons would also help users understand them if they know lex already.</para>
	</chapter>
-->

	<appendix id="appendix">
		<title>Appendix</title>

		<!-- TODO: maybe not in the guide: in an examples/ directory
			perhaps?

		<section>
			<title>Example descriptions</title>

			<section>
				<title>Comment stripping</title>

				<para>A C comment stripper. It needs to be aware of
					string literals (so it can avoid comment delimiters inside them),
					characters constants (ditto) and comments (so it can strip
					them).</para>
			</section>

			<section>
				<title><acronym>RPN</acronym> Calculator</title>
			</section>

			<section>
				<title>Non-<acronym>RPN</acronym> Calculator</title>

				<para>TODO: joint example with sid.</para>
			</section>
		</section>
		-->

		<!--
		<section>
			<title>Description grammar</title>

			Grammar (for reference) and sequence of keywords, reserved words, etc
				This probably makes more sense for the next version, really
		</section>
		-->

		<section>
			<title>Undefined Behaviour</title>

			<para>Undefined behaviours are actions that generate output which may
				be invalid, nonsensical, undesired or simply legal but obscure.</para>

			<para>The following constructs are syntactically legal input to
				&lexi.app;, but produce
				effects which are undefined behaviour. They may be disallowed
				entirely in future versions and should be avoided. This particular
				release of &lexi.app; permits these as
				undefined to offer a transition period only.</para>

			<!-- TODO: list undefined behaviours -->
			<variablelist>
				<varlistentry>
					<term>Mapping to the start of another mapping</term>

					<listitem>
						<para>To define a mapping which produces
							a character used at the start of any mapping, including
							itself. For example:<!-- TODO: example of both --></para>
							<!-- TODO: to identify this, we add a stage after the table is
								constructed which checks each mapping. -->

						<programlisting language="lxi">MAPPING "???" -> "?" ;</programlisting>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term>Calling one function from multiple different-length tokens</term>

					<listitem>
						<para>For example:</para>

						<programlisting language="lxi">TOKEN "//" -> get_comment () ;
TOKEN "#"  -> get_comment () ;</programlisting>

						<para><!-- Would make calls to <code>get_comment('/','/');</code> and
							<code>get_comment('#');</code> respectively.
							The difference in the number of arguments
							may cause a problem. -->One workaround is to call
							two macros of different prototypes which call
							the same function.</para>
					</listitem>
				</varlistentry>
			</variablelist>
		</section>

		<section>
			<title>Obscure Features</title>

			<para>This section describes legal constructs of doubtful
				interest.</para>

			<!-- TODO: Kevin says:
            TOKEN "&amp;&amp;" -> and;

       are legal tokens' definitions. The first one lacks the $ sign and is of
       no  interest  if  you use the sid parser generator, and lexi is usually
       meant to be used with sid.
	TODO: See above re lex_
	This is redundant now that we're going to disallow non-sid identifiers. When we do, we should rename them.
			-->

			<section>
				<title>Ranges Within Tokens</title>

				<para>The grammar for &lexi.app; permits
					strings formed of pre-defined character ranges to be used
					for token definitions:</para>

				<programlisting language="lxi">TOKEN {A-Z} + "something" -> $x ;</programlisting>

				<para>Character ranges are intended
					to be used for sets (that is, in <code>GROUP</code>), not
					sequences. Since tokens are defined using sequences, this
					really means:</para>

				<programlisting language="lxi">TOKEN "ABCDEFGHIJKLMNOPQRSTUVWXYZsomething" -> $x ;</programlisting>

				<para>This is probably not the intended effect.<!-- TODO: disallow
					ranges for tokens, then! eventually we can remove from
					grammar. disallow after first release. --> Using a group
					is suggested instead:</para>

				<programlisting language="lxi">GROUP alpha = {A-Z} ;
TOKEN "[alpha]something" -> $x ;</programlisting>
			</section>

			<section>
				<title>Whitespace Within Tokens and Keywords</title>

				<para>The <quote>white</quote> group may be used in
					tokens and keywords as any other group would:</para>

				<programlisting language="lxi">TOKEN "a[white]" -> $something ;
TOKEN "[white]ab" -> $neverscanned ;</programlisting>

				<para>The above are both legal tokens definitions.
					The second one will  never  be  scanned since
					white characters are discarded before tokens are
					matched.</para>	<!-- TODO: in the future make this
					illegal. -->

				<!--
				TODO: the first one is ok? odd! TODO: list precedence of these
				things! that should be a section by itself perhaps
				-->
			</section>
		</section>
	</appendix>

	<!-- TODO: include this in the global glossary as a per-project section. In a future version.
	<appendix id="glossary">
		<title>Glossary</title>

Token
Group
Set
List
Keyword
String
Range
	</appendix>
 -->
</book>

