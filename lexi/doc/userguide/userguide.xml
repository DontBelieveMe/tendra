<?xml version="1.0" standalone="no"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
"http://www.oasis-open.org/docbook/xml/4.4/docbookx.dtd">

<!--
  $Id$
-->

<book>
  <bookinfo>
    <title>The <application>Lexi</application> Users' Guide</title>

	<author>
		<firstname>Katherine</firstname>
		<surname>Flavel</surname>
		<affiliation><orgname>The TenDRA Project</orgname></affiliation>
	</author>
	<pubdate>2007</pubdate>

	<copyright>
		<year>2007</year>

		<holder>The TenDRA Project</holder>
	</copyright>
  </bookinfo>

<!-- TODO when writing manpage, we can take the opportunity to declare as BUGS
(or indeed as undefined) features which no .lxi file uses.
-->

	<!--
	   - I have 1. designed the structure of my document, 2. read the
	   - program's grammar and described each command in turn, 3. begun
	   - integrating a manpage for invocation Kevin reverse-engineered, and
	   - will 4. describe the algorithms and runtime it employs
	   -
	   - Don't forget to explain that this was inspired by Kevin's manpage
	   - (and indeed that we'll produce a new manpage from this guide),
	   - although his text was completely rewritten for this guide, it
	   - served as a helpful comparison.
	  -->

	<chapter id="introduction">
		<title>Introduction</title>

		<para><application>Lexi</application> translates a description of a lexical
			analyser into C code implementing that analyser. It aims to
			provide simple, straightforward features for lexical analysis,
			and to leave more complex behaviour to its calling program.</para>

		<para>This document describes how to use the <application>Lexi</application>
			lexer generator. It was written for <application>Lexi</application> version
			2.0.</para>

		<para>This version of <application>Lexi</application> (trunk pre 2.0) is the current
		       working. Once all new features have been added, we will tag a 2.0-proto-1
		       release. The 2.0 release will happen once we have frozen the syntax.
		       Backward compatibility is not preserved with regards to 1.3. However,
		       we are currently working on an option switch that would emulate old behavior.</para>

		<para>This document is a modification of the userguide for <application>Lexi 
		        </application> 1.3. <application>Lexi</application> 1.3
		        was an interim release to
			mark a stable release before forthcoming <acronym>API</acronym>
			changes scheduled for the next release. This provides no features
			over 1.2 save for some minor points (listed under the change
			history below); it serves mostly to collate the restructuring
			of the codebase during development; 1.3 is the first release of
			<application>Lexi</application> as a stand-alone product, separate to the
			TenDRA distribution.</para>

		<para>
		       Any original documentation <application>Lexi</application> once had
			previous to version 1.3 is lost. Since there is no surviving
			documentation, this guide has been written from scratch, based
			from observations of the source code. It is
			difficult to distinguish between obscure features and undefined
			behaviour; we are taking the opportunity here to explicitly
			state which features are undefined behaviour and which are
			intentional. We try to maintain backwards compatibility with
			these features, based on the <application>Lexi</application> files
			present in the TenDRA repository.</para>

		<!-- TODO remove obsoleted parts as each feature is changed -->

		<!-- TODO state why we're changing our api: see the
			"this is for version X" heading -->

		<section id="history">
			<title>Change History</title>

			<para>The main feature changes of each version of
				<application>Lexi</application> are listed below:

				<!-- TODO merge this with an external changelog, as for sid -->
				<variablelist>
					<title>Versions of <application>Lexi</application></title>

					<varlistentry>
						<term>Post 2.0</term>
						<listitem>
							<para>Buffers</para>
							<para>Counters</para>
							<para>Syntactic sugar for identifiers and comments</para>
						</listitem>

					</varlistentry>

					<varlistentry>
						<term>2.0</term>
						<listitem>
							<para>The // comment syntax is now recognized </para>
							<para> Zones have been added to lexi </para>
							<para> Default token syntax </para>
							<para> Keyword generation cleaned up from the previous hackish API </para>
							<para>A new input file: the lct file which let you specify headers, trailers and copyright statement for the generated files.</para>
							<para> Multiple function call upon encountering a token</para>
							<para> Argument control</para>
							<para> Outputs both a header and a C file</para>
							<para> Namespace control with prefixes for API and terminals</para>
							<para> SOON: actions that will closely look like their <application>sid</application> counterparts. Advantages over direct function calls include (subject to change) type checking, inlining (better performance). </para>
						</listitem>
					</varlistentry>

					<varlistentry>
						<term>1.3</term>
						<listitem>
							<para>Interim convenience release.
		<!-- TODO mostly similar to the lexi in TenDRA 4.2.1,
			save for the addition of // comments (?), and what else?
			- TODO no, it looks like i didn't add Kevin's patch for that. -->

								<itemizedlist>
									<listitem>
										<para>Separated from the TenDRA
											distribution to form a stand-alone
											project.</para>
									</listitem>
									<listitem>
										<para>Various behaviours undefined<!-- TODO --></para>
									</listitem>
								</itemizedlist>
							</para>
						</listitem>
					</varlistentry>

					<varlistentry>
						<term>1.2</term>
						<listitem>
							<para><code>PROTO</code> removed. This corresponds
								to Rob Andrew's patches, which will be applied
								in a future version instead. This version is
								skipped to avoid confusion and is tagged for
								internal use only.</para>
						</listitem>
					</varlistentry>

					<varlistentry>
						<term>1.1</term>
						<listitem>
							<para>This was the last version provided by the
								TenDRA 4.1.2 release.</para>
						</listitem>
					</varlistentry>
				</variablelist>

			</para>
		</section>
	</chapter>

	<chapter id="lexing">
		<title>Lexical Analysis</title>

		<section id="lexingoverview">
			<title>Overview</title>

			<para>Lexical analysis is the process of categorising an input
				stream into tokens. Each of these categories (called lexemes)
				are determined by the spelling of the contents of that token.
				For convenience of discussion, this guide assumes that
				tokens are formed of characters, although this is not
				necessarily the case.</para>

<!-- TODO maybe reword the above?
firstly define lexemes
<kate> the process of categorising words into each lexeme is called lexing
<kate> a program which performs lexing is called a lexer
<kate> this program generates lexers based on your set of lexemes
<kate> clearly defining every word would be quite tedious
<kate> so (say we have a lexeme called "number") you can say like "anything 
       consisting entirely of digits is a number"
-->


			<para>Categories of tokens are arranged such that spellings the
				possible sets of spellings forming each category are
				unambiguous. For example, commonly floating point literals are
				distinguishable from integer literals because they must contain
				a decimal point. If two different concepts must be expressed
				using the same (or overlapping) sets of spellings, a further
				stage after lexical analysis would be required to distinguish
				them. For example, commonly an identifier may represent either
				a variable or a function name, and it is the role of a parser
				to distinguish between them based on the ordering of the
				tokens.</para>

			<para>Usually it is the responsibility of the lexical analyser
				to skip whitespace (and often also comments).
				<xref linkend="tokenstream"/> illustrates an example of
				tokenisation:</para>

			<figure id="tokenstream">
				<title>Forming a Token Stream</title>
				<graphic align="center" format="PNG"
					fileref="tokens.png"/>
			</figure>

			<para>In this example, the lexer skips over whitespace characters
				(indicated in the input text by grey with a dotted underline),
				and categorises the remaining characters (indicated with a cyan
				background and a solid underline) according to their spelling. For
				example, a series of digits is categorised into the
				<literal>&lt;literal&gt;</literal> type. Notice that whitespace
				is optional; there happens to be non between the
				<quote><literal>53</literal></quote> and
				<quote><literal>;</literal></quote> tokens.</para>

			<para>The spelling of the characters which form each token is
				passed on, but usually ignored for punctual tokens such
				as statement separators and assignment operators. It is
				more relevant for the spelling to be known for
				identifiers, literals and other non-punctual tokens.</para>

			<para>The traditional use for a lexer is to feed this stream
				of categorised tokens to a parser; lexers are unaware of the
				order of tokens, since they are concerned with
				categorisation only.  The process of parsing then asserts
				that these tokens are in an order permitted by
				the given grammar. Usually storing the spellings associated with each
				tokens is a side-effect of the parse, resulting in a
				symbol table or equivalent (this depends on the purpose
				of the parser). Parsing is beyond the scope of this document,
				and will not be discussed here - see the Sid users' guide
				for details.</para>

			<para>For simple uses, a parser may not be required at
				<!-- TODO make examples, and mention them here -->all.</para>
		</section>

		<section id="concepts">
			<title>Concepts Lexi Expresses</title>

			<!-- TODO -->
			<variablelist>
				<varlistentry>
					<term><xref linkend="keyworddefinitions"/></term>

					<listitem>
						<para>Keywords are sequences of inseparable characters
							treated whole, indented to be passed back to the
							calling program as single tokens. These differ from
							most tokens in that their spellings are fixed, rather
							than a set of possible spellings.</para>

						<para><application>Lexi</application> itself does not perform
							processing of keywords; instead it outputs their
							definitions separately (see the <option>-k</option>)
							and these definitions are intended to be checked
							by a token calling a function.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><xref linkend="prepassmappings"/></term>

					<listitem>
						<para>For mappings of sequences of characters to a
							single character, pre-pass substitutions offer
							a mechanism to implement effects similar to
							trigraphs in C.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><xref linkend="whitespacedefinitions"/></term>

					<listitem>
						<para>Whitespace serves both to separate tokens who's
							concatenated spellings would be ambiguous (e.g. to
							distinguish <quote>abc</quote> and
							<quote>xyz</quote> from  <quote>abcxyz</quote>),
							and primarily to improve readability of the source.
							Whitespace is not passed on in the generated token
							stream.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><xref linkend="groupdefinitions"/></term>

					<listitem>
						<para>Group definitions define sets of characters which
							may be specified in place of a single character
							in tokens. The presence of a character in a group
							may also be tested via the generated
							<acronym>API</acronym>, much like C's
							<filename>ctype.h</filename> functions.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><xref linkend="tokendefinitions"/></term>

					<listitem>
						<para>Token definitions specify the set of spellings
							which form a lexeme. These are central to
							lexical analysis.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><xref linkend="conditionalcommands"/></term>

					<listitem>
						<para>Conditional commands provide a mechanism for
							optionally omitting features of the specified
							lexical analyser by way of runtime tests.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><xref linkend="defaultdefinition"/></term>

					<listitem>
						<para>Default definitions specify which action
						  should we performed upon encountering a non recognized 
						  token.</para>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term><xref linkend="zonedefinitions"/></term>

					<listitem>
						<para>Zone definitions specify zones in which 
						        the lexical analysis is subject to different rules.
						        Zones definition may contain token definitions, 
						        default definition, group definitions and whitespace 
						        definitions.</para>
					</listitem>
				</varlistentry>
			</variablelist>

			<para>Prior to 2.0, zones and default did not exist and 
			        any other facilities was provided by matching the start of
				a token and passing control to an externally defined function,
				This separation provided a general mechanism to allow the user
				to handle situations unforeseen by the authors, without making
				<application>Lexi</application> itself over complex by attempting to provide
				built-in mechanisms to express every corner-case (for example,
				switching to a "comment-parsing" mode). See the <!-- TODO -->
				function calls section for details. In 2.0, zones, at least once buffers are
			        implemented, should allow to express most of these possibililities even
			        ones not foreseen by the authors.</para>
		</section>


<!-- TODO in a future version
		<section id="implementation">
			<title>Lexi's Implementation</title>

			<para>Here we talk about how the generated lexical analysers
				work; how they use a look-up table for speed, etc. Note that
				they read a byte at a time: perhaps mention the ignorance of
				Unicode. TODO feature request: perhaps set Unicode (or other
				charset mapping) to UTF-8 as a pass phase<para>

			<para>TODO talk about passes </para>

			<para>TODO see section on performance.</para>
		</section>
-->

		<section id="invocation">
			<title>Invocation</title>

			<para><application>Lexi</application> is invoked with the form:</para>

			<xi:include xmlns:xi="http://www.w3.org/2001/XInclude"
				href="../cmdsynopsis.xml"/>

			<para>The <replaceable>output-file</replaceable> written is
				either an implementation of the
				lexical analyser specified, or if <option>-k</option> is
				given, a set of calls to define the keywords used.</para>

			<para>The <replaceable>input-file</replaceable> is the
				specification of the lexical analyser to generate.
				See <xref linkend="specification"/> for details of the
				format of this file.</para>

			<para>The <option>-l</option> specifies the output language.
			At the moment, only C90 and C99 are supported. </para>

			<para>The <option>-p</option> parameter specifies a prefix
				to all the function provided by the API. This defaults to
				<literal>lexi_</literal>. This prefix is also used for
			        calling function that must be provided by the user:
			        i.e. <programlisting language="C">lexi_readchar</programlisting></para>

			<para>The <option>-t</option> parameter specifies a prefix
				to the identifiers of tokens returned; this defaults to
				<literal>lex_</literal>.</para>

			<para>For details of the options available, see the
				<application>Lexi</application> manpage.</para>
		</section>
	</chapter>

	<xi:include xmlns:xi="http://www.w3.org/2001/XInclude"
		href="specification.xml"/>

	<xi:include xmlns:xi="http://www.w3.org/2001/XInclude"
		href="commands.xml"/>

	<chapter id="interface">
		<!-- TODO in the future this should be separate chapters, one per language supported -->
		<title>Interface</title>

		<para><application>Lexi</application> generates and outputs C89 code
			implementing the specified lexical analyser. This code
			is intended to be linked as part of a larger C program
			by the user.</para>

		<!-- TODO state size limits of things in a "limits" section. Exceeding limits is undefined -->

		<section>
			<title>Instantiation</title>

			<para>Input to the generated lexical analyser is by way of
				a single function (or macro, if desired) that the user is
				expected to provide to read a character:</para>

			<programlisting language="C">int read_char(void);</programlisting>

			<para>Output from the lexical analyser is by way of a function
				provided by <application>Lexi</application> that returns the next
				token to the user's program:</para>

			<programlisting language="C">int read_token(void);</programlisting>

			<para>This is the user's primary interface to the lexical
				analyser. Note that the type of characters is <code>int</code>,
				so that <literal>EOF</literal> may be expressed if
				neccessary.</para>

			<para>Calling <code>read_token()</code> will result
				in the lexical analyser
				making as many calls to <code>read_char()</code>
				as are necessary.</para>

    <!-- TODO talk about read_char_aux() used for MAPPNG.
        read_char_aux() is called in place of read_char().
    -->
		</section>

		<section id="interface_terminals">
			<title>Terminals</title>

			<para><application>Lexi</application> does not define the values of
				terminals in the generated code; these are expected to
				be specified by the user (most usually from a
				parser-generator such as <application>Sid</application>).
				For example, a token defined by:</para>

			<programlisting language="Lexi">TOKEN "--" -> $dash ;</programlisting>

			<para>Would return the value of <literal>lex_dash</literal>
				from <code>read_token()</code> when matched. The prefix of
				these identifier names may be specified with the
				the <option>-l</option> option.
				See the <application>Sid</application> documentation for
				further discussion of the C representation of
				<application>Sid</application>'s terminals.</para>

			<!-- TODO state the API. ditto sid. include macros etc. (done?) -->

			<!-- TODO talk about how the actual values are returned
				with tokens (tags); they're not. you're expected to
				do so yourself -->
		</section>

		<section id="interface_functions">
			<title>Functions</title>
			<!-- TODO we could express system-defined functions as $x() perhaps -->

			<para>Within the C implementation of functions, the usual
				<application>Lexi</application> <acronym>API</acronym>
				functions may be called. For example, to call
				<code>read_char()</code>.
				This is especially useful for calling the functions
				defined to identify membership in groups. A common
				case is to read tokens of a variable length.
				This is especially suitable for reading identifiers.
				For example (where <code>unread_char()</code> is a
				user-defined function with the obvious effect):

				<programlisting language="Lexi">GROUP identstart = {a-z} + {A-Z} + "_";
GROUP identbody = "[identstart]" + {0-9} + "-";
TOKEN "[identstart]" -> read_identifier();</programlisting>

			<programlisting language="C">int read_identifier(int c) {
	for(;;) {
		if(c == EOF) {
			return lex_eof;
		}

		if(!is_identbody(lookup_char(c))) {
			unread_char(c);
			return lex_identifier;
		}

		/* store character here */

		c = read_char();
	}
}</programlisting>
				</para>

			<!-- TODO in the future when we remove the lookup_char function, you
				will be able to TOKEN "x" -> is_digit(). I don't know if that
				would be useful (since it returns an int), but mention it anyway -->

			<para>Functions called by tokens are passed each character forming the
				token. The example above would result in the call to:

				<programlisting language="C">get_identifier(c);</programlisting>

				where <code>c</code> is the content of the token (that is, the
				character matched by <code>"[identstart]"</code>. For multiple
				characters each character is passed:

				<programlisting language="Lexi">TOKEN "abc" -> f();</programlisting>
				<programlisting language="C">get_identifier('a', 'b', 'c');</programlisting>

				See <xref linkend="interface"/> for further details of the
				C interface the generated lexer will call.</para>

			<para>Note that it is undefined behaviour to have tokens of
				different lengths call the same function.</para>

		</section>

		<section>
			<title>Character Groups</title>

			<para>Should the user wish to check if a character is
				in a group, the generated code provides macros of
				the form <code>is_groupname()</code>. These are
				intended to be used as:</para>

			<programlisting language="C">is_digit(lookup_char(c))</programlisting>

			<para>assuming a group named <quote>digit</quote> is defined. See the
				<xref linkend="groupdefinitions" endterm="groupdefinitions.title"/>
				and <xref linkend="whitespacedefinitions"
				endterm="whitespacedefinitions.title"/> white sections for
				further details on group names.</para>
		</section>

		<section>
			<title>Keywords</title>
<!-- TODO mention MAKE_KEYWORD here. Give an example -->

			<para>Neither the keyword calls output by <option>-k</option> nor
				the lexical analyser itself depend on including any headers
				other than for the user's own code's requirements.</para>
		</section>
	</chapter>

<!-- TODO in a future version. probably discuss the trie here, too (just not the implementation of it)
	<chapter>
		<title>Implementation</title>

		<para>What is generated, and algorithms employed</para>

		<para>Performance relative to other lexers. We can port all our examples to lex and such, to compare.
			Side-by-side comparisons would also help users understand them if they know lex already.</para>
	</chapter>
-->

	<chapter id="appendix">
		<title id="appendix.title">Appendix</title>

		<!-- TODO maybe not in the guide: in an examples/ directory
			perhaps?

		<section>
			<title>Example descriptions</title>

			<section>
				<title>Comment stripping</title>

				<para>A C comment stripper. It needs to be aware of
					string literals (so it can avoid comment delimiters inside them),
					characters constants (ditto) and comments (so it can strip
					them).</para>
			</section>

			<section>
				<title><acronym>RPN</acronym> Calculator</title>
			</section>

			<section>
				<title>Non-<acronym>RPN</acronym> Calculator</title>

				<para>TODO joint example with sid.</para>
			</section>
		</section>
		-->

		<!--
		<section>
			<title>Description grammar</title>
			Grammar (for reference) and sequence of keywords, reserved words, etc
				This probably makes more sense for the next version, really
		</section>
		-->

		<section>
			<title>Undefined Behaviour</title>

			<para>Undefined behaviours are actions that generate output which may
				be invalid, nonsensical, undesired or simply legal but obscure.</para>

			<para>The following constructs are syntactically legal input to
				<application>Lexi</application>, but produce
				effects which are undefined behaviour. They may be disallowed
				entirely in future versions and should be avoided. This particular
				release of <application>Lexi</application> permits these as
				undefined to offer a transition period only.</para>

			<!-- TODO list undefined behaviours -->
			<variablelist>
				<varlistentry>
					<term>Mapping to the start of another mapping</term>

					<listitem>
						<para>To define a mapping which produces
							a character used at the start of any mapping, including
							itself. For example:<!-- TODO example of both --></para>
							<!-- TODO to identify this, we add a stage after the table is
								constructed which checks each mapping. -->

							<programlisting language="Lexi">MAPPING "???" -> "?" ;</programlisting>
					</listitem>
				</varlistentry>

				<varlistentry>
					<term>Calling one function from multiple different-length tokens</term>

					<listitem>
						<para>For example:

							<programlisting language="Lexi">
TOKEN "//" -> get_comment () ;
TOKEN "#"  -> get_comment () ;</programlisting>

						<!-- Would make calls to <code>get_comment('/','/');</code> and
							<code>get_comment('#');</code> respectively.
							The difference in the number of arguments
							may cause a problem. -->One workaround is to call
							two macros of different prototypes which call
							the same function.</para>
					</listitem>
				</varlistentry>
			</variablelist>
		</section>

		<section>
			<title>Obscure Features</title>

			<para>This section describes legal constructs of doubtful
				interest.</para>

			<!-- TODO Kevin says:
            TOKEN "&amp;&amp;" -> and;

       are legal tokens' definitions. The first one lacks the $ sign and is of
       no  interest  if  you use the sid parser generator, and lexi is usually
       meant to be used with sid.
	TODO See above re lex_
	This is redundant now that we're going to disallow non-sid identifiers. When we do, we should rename them.
			-->

			<section>
				<title>Ranges Within Tokens</title>

				<para>The grammar for <application>Lexi</application> permits
					strings formed of pre-defined character ranges to be used
					for token definitions:

					<programlisting language="Lexi">TOKEN {A-Z} + "something" -> $x ;</programlisting>

					Character ranges are intended
					to be used for sets (that is, in <code>GROUP</code>), not
					sequences. Since tokens are defined using sequences, this
					really means:

					<programlisting language="Lexi">TOKEN "ABCDEFGHIJKLMNOPQRSTUVWXYZsomething" -> $x ;</programlisting>
				</para>

				<para>This is probably not the intended effect.<!-- TODO disallow
					ranges for tokens, then! eventually we can remove from
					grammar. disallow after first release. --> Using a group
					is suggested instead:

					<programlisting language="Lexi">GROUP alpha = {A-Z} ;
TOKEN "[alpha]something" -> $x ;</programlisting>
				</para>
			</section>

			<section>
				<title>Whitespace Within Tokens and Keywords</title>

				<para>The <quote>white</quote> group may be used in
					tokens and keywords as any other group would:</para>

				<programlisting language="Lexi">
TOKEN "a[white]" -> $something ;
TOKEN "[white]ab" -> $neverscanned ;</programlisting>

				<para>The above are both legal tokens definitions.
					The second one will  never  be  scanned since
					white characters are discarded before tokens are
					matched.</para>	<!-- TODO in the future make this
					illegal. -->

				<!--
				TODO the first one is ok? odd! TODO list precedence of these
				things! that should be a section by itself perhaps
				-->
			</section>
		</section>
	</chapter>

	<!-- TODO include this in the global glossary as a per-project section. In a future version.
	<chapter id="glossary">
		<title>Glossary</title>

Token
Group
Set
List
Keyword
String
Range
	</chapter>
 -->
</book>

