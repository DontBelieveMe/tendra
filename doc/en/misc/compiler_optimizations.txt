# $TenDRA$
                        Online Compiler Baedeker

                                 being 

                    A Compilation of Compiler Notes

                        nonce at tendra dot org

|=----------------------------=[ E H ? ]=------------------------------=|

__        __
\ \      / /  hat follows is a set of notes on advanced compiler
 \ \ /\ / /   topics.  These notes are intended to serve as a 
  \ V  V /    'thinking piece' to explore various advanced in 
   \_/\_/     compiler technology.  The idea is to catalog major
              improvements, and determine if they are suitable
for inclusion in the TenDRA compiler.  You can learn more about
the TenDRA compiler by visiting http://www.tendra.org.


|=-----------------------=[ O V E R V I E W ]=--------------------------=|

 _____ 
|_   _| he following is a collection of notes on compiler
  | |   optimization techniques, with relevant references.  These
  | |   notes are culled from web searches, literature searches,
  |_|   experience, and a few good reference texts.  Allen & Kennedy's 
        excellent text "Optimizing Compilers for Modern Architectures"
(ISBN 1-55860-286-0). provides a good overview, at least from a
dependency perspective.  Andrew Appel's "Modern Compiler
Implementation in ML" (ISBN 0-521-58274-1) provides a good overview of
basics.  J. Glenn Brookshear's "Formal Languages, Automata, and
Complexity" (ISBN 0-8053-0143-7) provides an *excellent* theoretical
background.

    For historical reference, I've consulted John A.N. Lee's "The
Anatomy of a Compiler" (Reinhold, 1967).  For inspiration, Christian
Queinnec's "Lisp In Small Pieces" (ISBN 0-521-56247-3), was consulted.

    Lastly, to 'keep up with the Jones'', I've read deeply of Serge
Lidin's "Inside Microsoft .NET IL Assembler" (ISBN 0-7356-1547-0), and
Robert Stark, et al., "Java and the Java Virtual Machine: Definition,
Verification, Validation" (ISBN 3-540-42088-6).  Stark's book on the
validation of the JVM, in particular, is most remarkable.  The future
of compilation, it would seem, includes proof-carrying or validation
of some sort; it's well worth the read.

    Together, these texts suggest the following organization of topics.
(At present, the outline is heavily skewed towards dependence
analysis, since that would appear to yield the best results for
TenDRA.)  Other organizing principles will be used in future
revisions.

|=----------------------=[ B A C K G R O U N D ]=------------------------=|

    These notes are intended for readers already skilled in basic
compiler technology.  If you are not well versed in compilers, I'd
recommend getting a bucket of sand, making an ingot of super pure
silicon, wafering the ingot, and etching CMOS circuits onto the
wafers.  Then, use low voltage power to drive a state machine etched
onto the chips.  I'd recommend a register based machine, but you can
use stacks for simplicity.  Now you're ready to create an instruction
set architecture on paper.  After settling on the ISA, make some
microcode to control your simple state machine, and Mab a few
prototypes.  Then you're ready to write some machine code.  After
wasting years of your life writing machine code, abstract the
code-writing process into a series of formal steps, borrowing heavily
from linguistics along the way.  Make higher level languages, and
tools to translate your ambiguous expressions into machine
code. You'll get the idea.

   The rest should be easy.

|=-----------------------------------------------------------------------=|

0.  Lessons from History

                   "And you may ask yourself: Well...How did I get here?"
                   -- D. Byrne

These section looks briefly at trends in computer architecture, and
the demands they place on compiler writings.


    A.  The Rise of Pipelining

    From the 1960s computer architecture has stressed the creation of
execution pipelines to increase efficiency.  The early IBM 7094, for
example used a two-stage pipeline of fetch and execute.  With multiple
stages, overlapping instructions were possible.  Modern RISC machines
add more stages.  The theoretical DLX, for example, used by Hennessy
and Patterson use five stages:

        -- Instruction Fetch (IF)
        -- Instruction Decode (ID)
        -- Execute (EX)
        -- Memory Access (MEM)
        -- Writeback (WB)

Finer granularity in stages allows for more overlapping, thus:

       1    2    3     4    5    6    7
     :    :    :    :     :    :    :    :
     :    :    :    :     :    :    :    :
     +----+----+----+-----+----+    :    :
     | IF | ID | EX | MEM | WB |    :    :
     +----+----+----+-----+----+----+    :
	  | IF | ID | EX | MEM | WB |    :
	  +----+----+----+-----+----+----+
	       | IF | ID | EX | MEM | WB |
	       +----+----+----+-----+----+
 
So, before a single instruction can complete, another one is started
down the pipeline.  The development of pipeline technologies prompted
the study of types of data hazards.  This yielded the familiar et of
hazard types:

       1) structural, which happen because of hardware
          limitations that do not support instruction overlap

       2) data hazards, which occur when on instruction needs
          the output of another (as yet) incomplete instruction

       3) control hazards, which occur because of branch
          handling.

In order to keep the pipeline full, and to possibly detect and avoid
hazards, compiler writers focused on instruction reordering.
Pipelining has thus become a mainstay of compiler technologies.  It's
become so ubiquitous that it's even studied in introductory computer
science courses.


    B.  The Rise of Fine Grained Parallelism

    Not every process can be pipelined.  Some operations, for example,
take several cycles to complete.  (Floating point math operations take
anywhere from 4 cycles (for multiplication) to 60 cycles or more (for
division).  Although this varies greatly with hardware, the general
point holds true: different instructions take different amounts of
time.

    As a result, not every piece of code can be pipelined.  A 5-stage
pipeline is not able to speed up multiple sequential floating point
operations, for example, since these typically take more cycles than
there are stages in a pipeline.  Computer engineers have noted this as
a source of many bottlenecks, and therefore introduced (also in the
1960s) parallel functional units.  Thus, a CPU might have 4 floating
point units, so that multiple operations can take place at once.

    With x functional units on a taking y cycles to complete, an
architecture could provide x/y output.  This gave rise to what is
known as "fine-grained parallelism".  

    There are a few problems with fine-grained parallel architectures.
Seymour Cray's work in the 1960s on the Control Data 6600 machines
discovered that pipelining was roughly equivalent to parallelism.
Moreover, complex control hardware was needed to manage the units
(e.g., Tomasulo's algorithm; see:

    http://www.dcs.ed.ac.uk/home/hase/projects/tomasulo/

and similar pages.)


    C. The Rise of Vector Instruction Machines

    The problem with pipeline machines was the cost of stalls.  If a
pipeline could not be kept full, the cost (particularly on deeply
pipelined architectures) could be significant.  The super computers of
the 1970s had specialized hardware to 'look ahead' in instruction
streams to try and detect hazards.  The complicated the IF stage, and
required additional hardware.

    As a result, computer engineers next looks to vector instruction
machines as a way to simplify IF.  Vector instructions operation on
two quantities in memory using fixed sized vector registers.  Combined
with vector chaining (where the partial output of one vector operation
is delivered to another waiting vector), vector instructions greatly
simplified issuance.

    But vector operations also came at a cost.  With additional
processor state, context switches became expensive.  Instruction
decode also became complex, given the large number of vector
instructions available.  Additionally, vector instructions complicated
cache management: their either prematurely evicted cache, or by-passed
cache altogether.  (E.g., the Cray machines often used no cache in
vector architectures, favoring numerous scalar temporary registers.)

    The growth of vector machines created a tremendous need for
compilers that recognized 'vectorizable code'.  Quite understandably,
software writers were reluctant to write explicit vectorizable code.
(E.g., most Fortran 90 coders will actually use a Fortran 77 subset,
to be safe, and make more portable code.)  Thus, it became the job of
compiler writes to identify loop structures that were the equivalent
of a vector instruction.

    The gave rise to the tremendous field of data dependence analysis.
Work in this area continues today.  It's a mature field, but one with
frequent innovations.


    D.  The Rise of Superscalar and VLIW Processors

    Vector machines were useful, but they complicated instruction set
design.  So, engineers returned to the original pipeline model, and
created two new innovations: superscalar architectures and VLIW
processors.  They are different, but have the same general idea: issue
instructions FAST.

    In a superscalar machine, hardware looks ahead to find
instructions that are ready for execution.  Sometimes, instructions
are even issued out of order (OOI).  VLIW processors issue multiple
instructions in the same cycle.  With 'wide instructions', multiple
parallel units are used.

    In VLIW machines, the task of the compiler writer is to pack
instructions tightly into each cycle, so that parallel functional
units are kept busy.  In a very basic sense, the compiler does the job
of the superscalar logic.  VLIW machines typically do not need
complicated look ahead hardware.

    The rise of these architectures has complicated the life of
compiler writers.  Since programmers do not typically write in
machine language, the compiler must identify data dependence
relations.


    E.  Lessons Learned

    History suggests the following tensions are at work:

    -- Programmers don't care about the hardware as much as we'd like.
       They typically write in a high level language for efficiency.
       With many exceptions (e.g., PFC) these languages do not expose
       features of the hardware.  The current direction of these
       languages seems to be to hide more details.  E.g., the trend in
       C is towards ANSI portability.

    -- The hardware is capable of out of order execution, yielding
       impressive speedups.  Some hardware supports OOI; however, the
       trend seems to be to have the compiler writers do this part of
       the job.  Data dependence analysis is key.

    -- Even programs that provide explicit representation of
       parallelism cannot guarantee optimal use of parallel hardware.
       Smart compilation is needed.

    Given the multitude of hardware, even within an architecture
family, an efficient, rich data representation is needed.  The lessons
learned in parallel fortran (PFC) suggest that internal data
representations are essential.  (This is perhaps a lesson that is
being re-learned by gcc's rtl.)

    Tendra uses a rather unique data representation format: the
Architecture Neutral Distribution Format (XANDF).  This andf
representation is as rich as any other uniform common language.
However, the andf format is also extensible in two explicit ways.
First, SORTs can be extended (as one might expect of a representation
that is agnostic about data format.)  Second, linkable entities and
units can be created to supply more information downstream to the code
generators ('installers' in TenDRA-speak.)

    Given the trends in hardware design, and the need to support a
growing number of architectures, it seems prudent to adopt and perhaps
extend the andf format.  It remains to be seen how much data
dependence can be expression in TenDRA.  These notes explore data
dependence in great detail, to help guide investigation and
experimentation.

I.  Dependence Analysis

    True enough, in its purest form, dependency testing presents an
undecidable problem.  One cannot dispute that.  Subscripted array
positions in array references can refer to arbitrary values, unknown
until runtime.  But in many cases, subscripts are simple form
polynomials of induction variables.  In such cases, subscripts
references are amenable to compile time analysis.  The key, of course,
is to structure an analysis of subscripts that recognizes and avoids
nonlinearity.

    Below, we examine a few techniques.

    A.  In General

        1.  Subscript Partitioning

    References for further reading:
        http://nr.stic.gov.tw/ejournal/ProceedingA/v23n6/751-765.pdf
        http://www.eecg.toronto.edu/~tsa/jasmine/adcp.html

        2.  Merging Direction Vectors

    B.  Single-Subscript Dependency

    Compiler writers use the term 'subscript' to refer to not one
subscript, but a pair of array references.  Since dependency testing
is concerned with relations between references, this makes sense.
Consider the following loop:

        DO i
          DO j
            DO k
               Array(i,j) = Array(i,k) * const;    // S_1
            OD
          OD
        OD

In performing dependency analysis of statement S_1, we say that the
first subscript of Array contains just i, while the second subscript
of Array contains both j and k.  (That is, in any dependency analysis,
we have to consider the values of j and k when looking at the second
subscript.)

    Subscript pairs are classified by the number of variables they
contain.  Some subscript pairs contain zero variables (or more
precisely, no variables that change within the loop structure).  These
'ZIV' subscripts include array constant references and references
using known variables.  Thus, the expression "Array(5) = Array(4) * x"
is a ZIV expression.

    If the subscript pair contains only one variable, the pair is
classified as a SIV expression.  Similarly, if multiple index
variables occur, the subscript pair is called MIV type.  Consider:

        DO i
          DO j
            DO k
               Array(3, i + 4, k) = Array(x, i, j) * const;    // S_1
            OD
          OD
        OD

In the expression S_1, the first subscript pair (the 3 and x) are
clearly ZIV types, since they don't use inductive loop variables.  The
second subscript pair (the 'i+4', and i pair) are SIV, since they both
use 'i', a loop variable.  The third pair is a MIV, since two loop
indices are used.

    Properly classifying expression subscripts is not merely an
academic undertaking.  There are numerous optimizations available,
once you know what type of subscript pairs are being used.

        1.  ZIV Test

        2.  SIV Test
 
        3.  Multiple Induction

    C.  Coupled Group Testing

        1.  Delta Tests

        2.  Other Tests

II.  Transformations

     A.  Loop Normalizations

     B.  Data Flow Analysis

         1.  Definition-Use Chains

         2.  Dead Code Elimination

         3.  Constant Propagation

         4.  Static Single-Assignment Form

     C.  Induction Variable Exposure

         1.  Forward Expression Substitution

         2.  Induction Variable Substitution

III.  Parallelism Enhancements--Fine Grained

    A.  Loop Interchange

    B. Scalar Expansion

    C. Scalar and Array Renaming

    D. Node Splitting

    E. Loop Skewing


IV.  Parallelism Enhancements--Course Grained

    A.  Single-Loop Methods

        1.  Privitization

        2.  Loop Distribution

        3.  Alignment

        4.  Code Replication

        5.  Loop Fusion

    B. Perfect Nested Loops

        1.  Interchange

        2.  Selection

        3.  Reversal

        4.  Skewing 

        5.  Unimodular Transformations

    C. Imperfect Nested Loop

        1.  Multilevel

        2.  Parallel Code Generation
      
V.  Control Flow Analysis

    A.  If-Conversion

        1.  What?

        2.  Branch Classification

            a.  Forward Branches

            b.  Exit Branches

            c.  Backward Branches

         3.  Complete Forward Branch Removal

    B. Control Dependence

VI. Register Use Management

    A.  Scalar Register Allocation

        1.  Data Dependence 

        2.  Loop-Carried & Loop-Independent Reuse

    References for further reading:

        http://nr.stic.gov.tw/ejournal/ProceedingA/v23n6/751-765.pdf

    B.  Scalar Replacement

        1.  Pruning Dependence Graphs

        2.  Simple Replacement

        3.  Handling Loop-Carried Dependencies

        4.  Spanning Dependencies

        5.  Eliminating Scalar Copies

        6.  Moderating Register Pressure

        7.  Scalar Replacement Algorithm

   C.  Unroll-and-Jam

         1.  Legality Consideration

         2.  The Unroll-and-Jam Algo

         3.  Effectiveness

    D.  Loop Interchange for Register Reuse

    E.  Loop Fusion

    F.  Other

        1.  Trapezoidal Loops

VII. Cache Use Management

     A.  Locality, Blocking, etc.










